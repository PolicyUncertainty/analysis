from io import StringIO

import matplotlib.pyplot as plt
import pandas as pd
import requests

from set_paths import create_path_dict

path_dict = create_path_dict()

from set_styles import set_plot_defaults

set_plot_defaults()

# =========
# PARAMETERS
# =========
MAJOR_GERMAN_OUTLETS = [
    "spiegel.de",
    "zeit.de",
    "faz.net",
    "sueddeutsche.de",
    "welt.de",
    "tagesschau.de",
    "handelsblatt.com",
    "bild.de",
    "focus.de",
    "stern.de",
    "tagesspiegel.de",
    "taz.de",
    "n-tv.de",
    "fr.de",
    "rp-online.de",
    "merkur.de",
    "berliner-zeitung.de",
]

GROUPS = {
    "Renteneintrittsalter_Debatte": [
        "Pension AND Age",
        "Retirement AND Age",
    ],
    "Abschlag_3_6": ["Pension AND Deduction", "Retirement AND Deduction"],
}

START_DATE = "2020-01-01"
END_DATE = "2025-10-17"


# ==========
# HELPER FUNCTION WITH DOMAIN FILTERING - SPLIT QUERIES
# ==========
def gdelt_query(keyword, start_date=START_DATE, end_date=END_DATE, chunk_size=8):
    """Query GDELT with German domain restriction, splitting into chunks to avoid URL length limits"""

    start_dt = start_date.replace("-", "") + "000000"
    end_dt = end_date.replace("-", "") + "235959"

    all_dfs = []

    # Split domains into chunks to avoid query length limits
    for i in range(0, len(MAJOR_GERMAN_OUTLETS), chunk_size):
        domain_chunk = MAJOR_GERMAN_OUTLETS[i : i + chunk_size]
        domain_filter = " OR ".join([f"domain:{d}" for d in domain_chunk])
        full_query = f"{keyword} ({domain_filter})"

        base_url = (
            f"https://api.gdeltproject.org/api/v2/doc/doc"
            f"?query={full_query.replace(' ', '+')}"
            f"&mode=TimelineVolRAW"
            f"&format=csv"
            f"&startdatetime={start_dt}"
            f"&enddatetime={end_dt}"
        )

        print(
            f"    Chunk {i // chunk_size + 1}/{(len(MAJOR_GERMAN_OUTLETS) - 1) // chunk_size + 1}...",
            end=" ",
        )

        try:
            response = requests.get(base_url, timeout=15)
            response.raise_for_status()
            df = pd.read_csv(StringIO(response.text))
            df = df[df["Series"] == "Article Count"].copy()

            if not df.empty:
                all_dfs.append(df)
                print(f"‚úÖ {len(df)} rows")
            else:
                print("‚ö†Ô∏è empty")

        except Exception as e:
            print(f"‚ùå Error: {e}")

    if not all_dfs:
        print(f"  ‚ö†Ô∏è No results for '{keyword}'")
        return None

    # Combine all chunks
    combined_df = pd.concat(all_dfs, ignore_index=True)

    # Group by date and sum values (in case same date appears in multiple chunks)
    combined_df["Date"] = pd.to_datetime(combined_df["Date"])
    combined_df = combined_df.groupby("Date", as_index=False)["Value"].sum()

    combined_df.rename(columns={"Date": "datetime", "Value": keyword}, inplace=True)

    print(
        f"  ‚úÖ Total: {len(combined_df)} days (range: {combined_df[keyword].min():.0f}-{combined_df[keyword].max():.0f})"
    )

    return combined_df


# ==========
# FETCH AND COMBINE DATA
# ==========
group_results = {}

for group, keywords in GROUPS.items():
    print(f"\nüìä Group: {group}")
    dfs = []

    for kw in keywords:
        df_kw = gdelt_query(kw)
        if df_kw is not None:
            dfs.append(df_kw)

    if not dfs:
        print(f"  ‚ö†Ô∏è No data for any keywords in this group")
        continue

    print(f"  ‚úÖ Successfully fetched {len(dfs)}/{len(keywords)} keywords")

    # Merge all keywords in the group
    merged = dfs[0]
    for df_kw in dfs[1:]:
        merged = pd.merge(merged, df_kw, on="datetime", how="outer")

    merged = merged.fillna(0)
    merged[group] = merged.drop(columns=["datetime"]).sum(axis=1)
    group_results[group] = merged[["datetime", group]]

# ==========
# CONTINUE WITH PLOTTING IF WE HAVE DATA
# ==========
if len(group_results) >= 2:
    combined = (
        pd.merge(
            group_results["Renteneintrittsalter_Debatte"],
            group_results["Abschlag_3_6"],
            on="datetime",
            how="outer",
        )
        .sort_values("datetime")
        .fillna(0)
    )

    # Apply 7-day rolling average
    combined["Renteneintrittsalter_Debatte_7Tage"] = (
        combined["Renteneintrittsalter_Debatte"].rolling(7, center=True).mean()
    )
    combined["Abschlag_3_6_7Tage"] = (
        combined["Abschlag_3_6"].rolling(7, center=True).mean()
    )

    # Plot
    fig, ax = plt.subplots()
    ax.plot(
        combined["datetime"],
        combined["Renteneintrittsalter_Debatte_7Tage"],
        label="Retirement Age",
        linewidth=2,
    )
    ax.plot(
        combined["datetime"],
        combined["Abschlag_3_6_7Tage"],
        label="ERP",
        linewidth=2,
    )

    ax.set_xlabel("Date")
    ax.set_ylabel("Average Daily Article Count (7-day MA)")
    ax.legend()
    ax.grid(True, alpha=0.3)
    fig.tight_layout()
    print(
        f"\n‚úÖ Analysis complete! Date range: {combined['datetime'].min()} to {combined['datetime'].max()}"
    )
    fig.savefig(
        path_dict["beliefs_plots"] + "media_comparison.png",
        bbox_inches="tight",
    )
else:
    print("\n‚ùå Not enough data to create comparison plot")
